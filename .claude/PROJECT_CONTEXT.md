# Web Automation & Research Project Context

## Project Overview

This project uses Claude Code's multi-agent system for advanced web automation, research, and site interaction tasks. The system leverages specialized agents to handle complex workflows involving:

- **Web Research & Information Gathering** - Finding and synthesizing information from the internet
- **Web Scraping & Data Extraction** - Extracting structured data from websites
- **Account Management & Authentication** - Logging into sites, creating accounts, managing credentials
- **Website Crawling & Navigation** - Navigating multi-page websites and following links
- **Documentation & Context Management** - Organizing findings and maintaining project context

## Environment

**Platform:** Claude Code Web (Cloud-based)

**Constraints:**
- All automation must work in cloud/serverless environments
- No persistent local storage between sessions
- Limited execution time for operations
- Must use cloud-compatible Python libraries

**Python Libraries Available:**
- `requests` / `httpx` - HTTP client libraries
- `beautifulsoup4` - HTML parsing
- `playwright-python` - Headless browser automation
- `lxml` - XML/HTML processing
- Standard library modules

## Installed Agents

### ğŸ¯ Core Orchestration & Research

1. **multi-agent-orchestrator** - Coordinates multiple specialized agents for complex workflows
2. **web-research-specialist** - Expert at web searches, information gathering, and synthesis
3. **web-automation-engineer** - Python-based web scraping, crawling, and automation

### ğŸ” Research Team

4. **research-orchestrator** - Coordinates multi-domain research projects
5. **research-synthesizer** - Synthesizes findings from multiple sources
6. **technical-researcher** - Deep technical documentation and API research
7. **search-specialist** - Optimizes search queries and strategies

### ğŸ“ Documentation & Context

8. **technical-writer** - Creates clear technical documentation
9. **documentation-expert** - Expert at documentation strategy and organization
10. **context-manager** - Manages project context and information flow

### ğŸ’» Development Support

11. **frontend-developer** - Frontend development and UI work
12. **backend-architect** - Backend architecture and API design
13. **fullstack-developer** - Full-stack application development
14. **code-reviewer** - Code quality and best practices review
15. **debugger** - Debugging and troubleshooting expert
16. **error-detective** - Error analysis and resolution

### ğŸ¤– AI & Tooling

17. **prompt-engineer** - AI prompt optimization
18. **task-decomposition-expert** - Breaking down complex tasks
19. **agent-expert** - Creating and managing specialized agents
20. **command-expert** - Creating custom Claude Code commands
21. **mcp-expert** - Model Context Protocol integrations
22. **cli-ui-designer** - Command-line interface design
23. **docusaurus-expert** - Docusaurus documentation sites

## Common Workflows

### 1. Research â†’ Implementation Pipeline

```
User Request
    â†“
multi-agent-orchestrator
    â†“
â”œâ”€â†’ web-research-specialist (gather information)
â”‚       â†“
â”‚   research-synthesizer (analyze findings)
â”‚       â†“
â””â”€â†’ web-automation-engineer (implement solution)
    â†“
documentation-expert (document results)
```

### 2. Web Scraping & Data Extraction

```
User Request
    â†“
web-automation-engineer
    â”œâ”€â†’ Analyze target website
    â”œâ”€â†’ Identify authentication requirements
    â”œâ”€â†’ Write Python scraping code
    â”œâ”€â†’ Extract and structure data
    â””â”€â†’ Export results (JSON/CSV)
```

### 3. Account Management Automation

```
User Request
    â†“
web-automation-engineer
    â”œâ”€â†’ Navigate to registration page
    â”œâ”€â†’ Fill out forms with provided data
    â”œâ”€â†’ Handle CSRF tokens and validation
    â”œâ”€â†’ Submit and verify account creation
    â””â”€â†’ Save credentials securely (env vars)
```

### 4. Multi-Source Research

```
User Request
    â†“
research-orchestrator
    â†“
â”œâ”€â†’ web-research-specialist (web sources)
â”œâ”€â†’ technical-researcher (docs/APIs)
â””â”€â†’ search-specialist (optimized queries)
    â†“
research-synthesizer
    â†“
technical-writer (create report)
```

## Best Practices

### Web Automation

- **Use headless browsers** (`playwright-python`) for JavaScript-heavy sites
- **Implement rate limiting** - Respect website resources (1-2 sec delays)
- **Check robots.txt** - Follow site crawling guidelines
- **Handle errors gracefully** - Implement retry logic with exponential backoff
- **Save state frequently** - Use checkpoints for resumable operations
- **Use environment variables** - Never hardcode credentials

### Research

- **Validate sources** - Use 4-tier credibility framework
- **Cross-reference** - Verify information across multiple sources
- **Cite properly** - Include URLs and confidence levels
- **Synthesize insights** - Don't just aggregate, analyze
- **Identify gaps** - Note what information is missing

### Context Management

- **Update PROJECT_CONTEXT.md** - Keep this file current with progress
- **Use TodoWrite** - Track multi-step tasks transparently
- **Document findings** - Create markdown summaries of discoveries
- **Save credentials** - Use `.env` files (gitignored) for sensitive data
- **Export data** - Save scraped data in structured formats (JSON/CSV)

## Project Structure

```
/home/user/claude-code-templates/
â”œâ”€â”€ .claude/
â”‚   â”œâ”€â”€ PROJECT_CONTEXT.md      # This file
â”‚   â”œâ”€â”€ agents/                 # Installed specialized agents (24)
â”‚   â”œâ”€â”€ scripts/                # Python automation scripts
â”‚   â””â”€â”€ data/                   # Scraped data and exports
â”œâ”€â”€ .env                        # Environment variables (gitignored)
â”œâ”€â”€ research/                   # Research findings and reports
â””â”€â”€ automation/                 # Web automation scripts
```

## Security & Ethics

### Allowed Activities
âœ… Authorized web scraping (public data, respecting robots.txt)
âœ… Account management for personal/authorized accounts
âœ… Research and information gathering from public sources
âœ… Automation of repetitive manual tasks
âœ… Testing and educational purposes

### Prohibited Activities
âŒ Scraping sites that explicitly prohibit it (robots.txt)
âŒ Creating fake/fraudulent accounts
âŒ Bypassing authentication or security measures
âŒ Excessive requests that could harm site performance
âŒ Scraping personal/private data without authorization
âŒ Violating terms of service

## Getting Started

### 1. Simple Web Research Example

```
Hey @multi-agent-orchestrator, I need to research the best Python web
scraping libraries for cloud environments and create a comparison table.
```

### 2. Login Automation Example

```
Hey @web-automation-engineer, I need to automate logging into
example.com with username/password authentication. The credentials
are in my .env file.
```

### 3. Data Extraction Example

```
Hey @web-automation-engineer, scrape the product listings from
catalog.example.com (pages 1-10) and export to CSV with:
product name, price, rating, and URL.
```

## Tips for Success

1. **Start with research** - Use `@web-research-specialist` before implementing
2. **Break down tasks** - Let `@multi-agent-orchestrator` coordinate complex workflows
3. **Test incrementally** - Start with single pages before batch operations
4. **Handle edge cases** - Account for missing data, timeouts, and errors
5. **Document progress** - Keep context updated for future sessions
6. **Use checkpoints** - Save progress for resumable long-running operations

## Session Workflow

For each task:

1. **Planning** - Describe what you want to accomplish
2. **Research** - Gather necessary information (if needed)
3. **Implementation** - Write and run automation code
4. **Validation** - Verify results and handle errors
5. **Documentation** - Save findings and update context
6. **Iteration** - Refine based on results

---

**Last Updated:** 2025-11-28
**Environment:** Claude Code Web (Cloud)
**Primary Use Cases:** Web research, scraping, automation, account management
